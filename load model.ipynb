{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08db55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9d78e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed53742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = json.load(open('config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ffaaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Into folder: Dataset/Not/Argetina\n",
      "\tSegmentize file: CBERS_4A_WPM_20210416_219_155_L2_BAND1.tif\n",
      "\t\tFile Shape: (14879, 14498, 3)\n",
      "\t\tImages div: 1/537932, 0%\n",
      "\t\tImages div: 53794/537932, 10%\n",
      "\t\tImages div: 107587/537932, 20%\n",
      "\t\tImages div: 161380/537932, 30%\n",
      "\tSegmentize file: CBERS_4A_WPM_20210416_219_155_L2_BAND2.tif\n",
      "\t\tFile Shape: (14879, 14498, 3)\n",
      "\t\tImages div: 1/537932, 0%\n",
      "\t\tImages div: 53794/537932, 10%\n",
      "\t\tImages div: 107587/537932, 20%\n",
      "\t\tImages div: 161380/537932, 30%\n",
      "\tSegmentize file: CBERS_4A_WPM_20210416_219_155_L2_BAND3.tif\n",
      "\t\tFile Shape: (14879, 14498, 3)\n",
      "\t\tImages div: 1/537932, 0%\n",
      "\t\tImages div: 53794/537932, 10%\n",
      "\t\tImages div: 107587/537932, 20%\n",
      "\t\tImages div: 161380/537932, 30%\n",
      "\tSegmentize file: CBERS_4A_WPM_20210416_219_155_L2_BAND4.tif\n",
      "\t\tFile Shape: (14879, 14498, 3)\n",
      "\t\tImages div: 1/537932, 0%\n",
      "\t\tImages div: 53794/537932, 10%\n",
      "\t\tImages div: 107587/537932, 20%\n",
      "\t\tImages div: 161380/537932, 30%\n",
      "\tJoinBands\n",
      "Into folder: Dataset/Yes/Argentina\n",
      "\tSegmentize file: CBERS_4A_WPM_20210119_224_147_L4_BAND1.tif\n",
      "\t\tFile Shape: (14582, 14193, 3)\n",
      "\t\tImages div: 1/516861, 0%\n",
      "\t\tImages div: 51687/516861, 10%\n",
      "\t\tImages div: 103373/516861, 20%\n",
      "\t\tImages div: 155059/516861, 30%\n",
      "\tSegmentize file: CBERS_4A_WPM_20210119_224_147_L4_BAND2.tif\n",
      "\t\tFile Shape: (14582, 14193, 3)\n",
      "\t\tImages div: 1/516861, 0%\n",
      "\t\tImages div: 51687/516861, 10%\n",
      "\t\tImages div: 103373/516861, 20%\n",
      "\t\tImages div: 155059/516861, 30%\n",
      "\tSegmentize file: CBERS_4A_WPM_20210119_224_147_L4_BAND3.tif\n",
      "\t\tFile Shape: (14582, 14193, 3)\n",
      "\t\tImages div: 1/516861, 0%\n",
      "\t\tImages div: 51687/516861, 10%\n",
      "\t\tImages div: 103373/516861, 20%\n",
      "\t\tImages div: 155059/516861, 30%\n",
      "\tSegmentize file: CBERS_4A_WPM_20210119_224_147_L4_BAND4.tif\n",
      "\t\tFile Shape: (14582, 14193, 3)\n",
      "\t\tImages div: 1/516861, 0%\n",
      "\t\tImages div: 51687/516861, 10%\n",
      "\t\tImages div: 103373/516861, 20%\n",
      "\t\tImages div: 155059/516861, 30%\n",
      "\tJoinBands\n",
      "Total de data input: 316439\n",
      "Dimension data input: (20, 20, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "data_eval = dataset.download_dataset(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf51130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = dataset.Normalizer.transform(data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a36124",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = dataset.split_data_x_y(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac84111",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_eval = dataset.TimeSeriesDataset(data_x, data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fcc6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_eval = DataLoader(dataset_eval, CONFIG[\"training\"][\"batch_size\"], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caead843",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = algorithm.CNNModel(config_model=CONFIG[\"model\"], config_training=CONFIG[\"training\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ec09d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet.load_state_dict(torch.load(CONFIG[\"model\"][\"path\"], map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fad46215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 316439 test images: 76.7114672970146 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (images, labels) in enumerate(dataloader_eval):\n",
    "        images = images.to(CONFIG[\"training\"][\"device\"])\n",
    "        labels = labels.to(CONFIG[\"training\"][\"device\"])\n",
    "        outputs = convnet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the {0} test images: {1} %'.format(total, 100*correct/total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
